{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 50 articles from 5 categories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Articles: 100%|██████████| 50/50 [00:17<00:00,  2.85 articles/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# configuration\n",
    "base_url = 'https://vietnamnet.vn/'\n",
    "# Supported categories: 'chinh-tri', 'thoi-su', 'kinh-doanh', 'van-hoa', 'giao-duc', 'the-gioi', 'the-thao', 'giai-tri', 'doi-song', 'suc-khoe', 'thong-tin-truyen-thong'\n",
    "categories = ['thoi-su', 'kinh-doanh', 'van-hoa', 'giao-duc', 'the-gioi']\n",
    "number_of_articles = 10 # for each category\n",
    "\n",
    "number_of_all_articles = len(categories) * number_of_articles\n",
    "categories_mapping = {\n",
    "    'chinh-tri': 'Chính Trị',\n",
    "    'thoi-su': 'Thời Sự',\n",
    "    'kinh-doanh': 'Kinh Doanh',\n",
    "    'van-hoa': 'Văn Hóa',\n",
    "    'giao-duc': 'Giáo Dục',\n",
    "    'the-gioi': 'Thế Giới',\n",
    "    'the-thao': 'Thể Thao',\n",
    "    'giai-tri': 'Giải Trí',\n",
    "    'chinh-tri': 'Chính Trị',\n",
    "    'doi-song': 'Đời Sống',\n",
    "    'suc-khoe': 'Sức Khỏe',\n",
    "    'thong-tin-truyen-thong': 'Thông tin và Truyền thông'\n",
    "}\n",
    " \n",
    "def get_article_links(base_url, url, max_articles):\n",
    "    \"\"\"\n",
    "    Returns a list of article links from the given url.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    while len(links) < max_articles:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('div', class_='horizontalPost__main'):\n",
    "            find_h3 = link.find_next('h3', class_='horizontalPost__main-title vnn-title title-bold')\n",
    "            find_a = find_h3.find_next('a')\n",
    "            article_link = urljoin(base_url, find_a['href'])\n",
    "            links.append(article_link)\n",
    "        page_list = soup.find('ul', class_='pagination__list')\n",
    "        link_next_page = page_list.find_next('li', class_='pagination__list-item').find_next('a')\n",
    "        if link_next_page is None:\n",
    "            break\n",
    "        url = urljoin(url, link_next_page.get('href'))\n",
    "    return links[:max_articles]\n",
    "\n",
    "def get_article_content(links):\n",
    "    \"\"\"\n",
    "    Returns a list of article content from the given links.\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    newline_regex = re.compile(r'\\n+')\n",
    "    for link_number, link in enumerate(links, start=1):\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            response.raise_for_status() \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            find_content = soup.find('div', class_='main-v1 bg-white')\n",
    "            if find_content:\n",
    "                title_raw = find_content.find('h1', class_='content-detail-title')\n",
    "                title = title_raw.get_text() if title_raw else \"\"\n",
    "                \n",
    "                content = find_content.find('div', class_='maincontent main-content') or \\\n",
    "                          find_content.find('div', class_='maincontent main-content content-full-image content-full-image-v1')\n",
    "                content_text = content.get_text() if content else \"\"\n",
    "                content_text = newline_regex.sub('\\n', content_text)\n",
    "                if title and content_text:\n",
    "                    article = {\n",
    "                        \"title\": title,\n",
    "                        \"content\": content_text\n",
    "                    }\n",
    "                    articles.append(article)\n",
    "                else:\n",
    "                    print(f\"\\nCannot find title or content for article number {link_number} ({link}), skipping...\")\n",
    "            else:\n",
    "                print(f\"\\nCannot find content for article number {link_number} ({link}), skipping...\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nError fetching {link_number} {link}: {str(e)}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def main(categories, number_of_articles, progress_bar):\n",
    "    result_directory = \"result\"\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.makedirs(result_directory)\n",
    "\n",
    "    for category in categories:\n",
    "        category_directory = os.path.join(result_directory, category)\n",
    "        if not os.path.exists(category_directory):\n",
    "            os.makedirs(category_directory)\n",
    "\n",
    "        url = urljoin(base_url, category)\n",
    "        links = get_article_links(base_url, url, number_of_articles)\n",
    "        articles = get_article_content(links)\n",
    "\n",
    "        for count, article in enumerate(articles, start=1):\n",
    "            article_file_name = f\"{count}.txt\"\n",
    "            article_file_path = os.path.join(category_directory, article_file_name)\n",
    "            while os.path.exists(article_file_path):\n",
    "                count += 1\n",
    "                article_file_name = f\"{count}.txt\"\n",
    "                article_file_path = os.path.join(category_directory, article_file_name)\n",
    "\n",
    "            with open(article_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{article['title']}\\n{article['content']}\")\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.refresh()\n",
    "                \n",
    "print(f\"Crawling {number_of_all_articles} articles from {len(categories)} categories...\")\n",
    "progress_bar = tqdm(total=number_of_all_articles, desc='Articles', unit=' articles',dynamic_ncols=True)\n",
    "main(categories, number_of_articles, progress_bar)\n",
    "progress_bar.close()\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
